---
import Base from '../../layouts/Base.astro';
import Breadcrumbs from '../../components/Breadcrumbs.astro';
import { comparisonPages, getComparisonBySlug, pickTools } from '../../lib/programmatic.js';

export async function getStaticPaths() { return comparisonPages.map((p) => ({ params: { slug: p.slug } })); }
const { slug } = Astro.params;
const page = getComparisonBySlug(slug);
if (!page) throw new Error(`Comparison page not found: ${slug}`);
const canonical = `https://aitraining.directory/compare/${slug}/`;
const candidates = pickTools((slug.length + 3) % 8, 4);

function formatToolName(part = '') {
  return part
    .split('-')
    .filter(Boolean)
    .map((chunk) => chunk.toUpperCase() === 'AI' ? 'AI' : chunk.charAt(0).toUpperCase() + chunk.slice(1))
    .join(' ');
}

function getComparisonLabels(currentSlug) {
  if (currentSlug.includes('-vs-')) {
    const [leftRaw, rightTail] = currentSlug.split('-vs-');
    const rightRaw = rightTail.split('-for-')[0];
    return { left: formatToolName(leftRaw), right: formatToolName(rightRaw) };
  }

  if (currentSlug.includes('-alternatives')) {
    const base = formatToolName(currentSlug.replace('-alternatives', ''));
    return { left: `${base} (current choice)`, right: 'Alternative options' };
  }

  return { left: 'Option A', right: 'Option B' };
}

const comparisonLabels = getComparisonLabels(slug);

const comparisonContentBySlug = {
  'chatgpt-vs-claude-for-ld-content': {
    decisionMatrix: [
      { criterion: 'Long-form policy rewriting quality', weight: '25%', whatGoodLooksLike: 'Assistant preserves intent, legal nuance, and audience readability in one pass.', leftLens: 'Strong at fast first drafts with broad prompt flexibility; verify tone consistency across long docs.', rightLens: 'Often stronger on structured, context-heavy rewrites; still run legal/compliance review before publish.' },
      { criterion: 'Prompt-to-output reliability for SMEs', weight: '20%', whatGoodLooksLike: 'SMEs can reuse one prompt template and get stable quality across modules.', leftLens: 'Performs well with concise prompt scaffolds and examples.', rightLens: 'Performs well when you provide explicit structure and role context.' },
      { criterion: 'Knowledge-base synthesis', weight: '20%', whatGoodLooksLike: 'Assistant can summarize multiple SOP sources into one coherent learning narrative.', leftLens: 'Good for rapid synthesis if source chunks are curated.', rightLens: 'Good for longer context windows and narrative continuity in dense docs.' },
      { criterion: 'Review + governance workflow', weight: '20%', whatGoodLooksLike: 'Outputs move through reviewer signoff with clear revision notes and version trails.', leftLens: 'Pair with external review checklist + change log for compliance-sensitive assets.', rightLens: 'Pair with the same checklist; score based on reviewer edit-load and cycle time.' },
      { criterion: 'Cost per approved module', weight: '15%', whatGoodLooksLike: 'Total cost decreases as approved module volume increases month over month.', leftLens: 'Model cost with your expected weekly generation + revision volume.', rightLens: 'Model the same scenario and compare cost to approved output, not draft count.' }
    ],
    buyingCriteria: [
      'Test one real SOP rewrite + one scenario-based lesson in both assistants using the same rubric.',
      'Track reviewer edit-load (minutes per module) as your primary quality metric.',
      'Create a shared prompt library so SMEs can reuse proven templates.',
      'Require source citation or reference notes for every factual claim in learner-facing copy.',
      'Choose the assistant that delivers lower revision burden over a 30-day pilot, not prettier first drafts.'
    ]
  },
  'heygen-vs-synthesia-for-training-videos': {
    decisionMatrix: [
      { criterion: 'Avatar realism and learner trust', weight: '20%', whatGoodLooksLike: 'Learners perceive delivery as credible and stay engaged through the full module.', leftLens: 'Evaluate presenter realism, emotional range, and pronunciation consistency for internal terminology.', rightLens: 'Evaluate the same signals plus whether templates remain consistent across departments.' },
      { criterion: 'Revision speed after SME feedback', weight: '25%', whatGoodLooksLike: 'Content owners can ship approved updates within one review cycle.', leftLens: 'Score how quickly teams can revise scenes, script timing, and visual emphasis.', rightLens: 'Score revision speed when edits span multiple lessons and recurring templates.' },
      { criterion: 'Localization + multilingual QA load', weight: '20%', whatGoodLooksLike: 'Regional language versions can be shipped with minimal manual clean-up.', leftLens: 'Test dubbing quality and pronunciation controls for role-specific vocabulary.', rightLens: 'Test language coverage, glossary control, and reviewer effort for multilingual rollouts.' },
      { criterion: 'Governance and enterprise readiness', weight: '20%', whatGoodLooksLike: 'Approval routing, workspace controls, and audit trails are clear for compliance reviews.', leftLens: 'Validate permissioning model and revision traceability for cross-functional teams.', rightLens: 'Validate equivalent controls and how easily reviewers can sign off in-platform.' },
      { criterion: 'Cost per published training minute', weight: '15%', whatGoodLooksLike: 'Total production cost falls as module volume scales month over month.', leftLens: 'Model spend using your planned lesson volume + localization footprint.', rightLens: 'Run the same model and compare against approved output velocity, not draft volume.' }
    ],
    buyingCriteria: [
      'Run one controlled pilot with the same SOP source and the same reviewer panel in both tools.',
      'Track learner-facing QA defects (pronunciation, pacing, visual mismatch) per published minute.',
      'Require at least one multilingual module in pilot scope before final selection.',
      'Document who owns script QA, media QA, and final compliance signoff after go-live.',
      'Select the platform that achieves lower revision burden and faster publish cadence over 30 days.'
    ]
  },
  'ai-dubbing-vs-subtitles-for-compliance-training': {
    decisionMatrix: [
      { criterion: 'Regulatory clarity for critical terms', weight: '25%', whatGoodLooksLike: 'Learners in every region interpret policy-critical wording consistently and pass scenario checks.', leftLens: 'Test dubbing accuracy for legal terminology, acronym pronunciation, and phrasing that could change compliance interpretation.', rightLens: 'Test subtitle wording precision for policy-critical statements and confirm readability against regional language standards.' },
      { criterion: 'Speed to publish after policy updates', weight: '25%', whatGoodLooksLike: 'Teams can ship approved language updates within SLA when regulations change.', leftLens: 'Measure turnaround from source-script change to QA-approved dubbed module across top languages.', rightLens: 'Measure turnaround from source-script change to approved subtitle package and LMS republish.' },
      { criterion: 'Learner comprehension in low-audio environments', weight: '20%', whatGoodLooksLike: 'Completion and assessment outcomes stay strong across office, field, and shift-based contexts.', leftLens: 'Evaluate whether dubbed narration improves comprehension for learners with limited reading bandwidth.', rightLens: 'Evaluate whether subtitle-first modules remain understandable where audio use is restricted or muted.' },
      { criterion: 'QA and governance overhead', weight: '15%', whatGoodLooksLike: 'Localization QA load is predictable with clear reviewer ownership and signoff evidence.', leftLens: 'Score reviewer minutes per locale for pronunciation checks, timing corrections, and re-export cycles.', rightLens: 'Score reviewer minutes per locale for translation checks, subtitle timing alignment, and legal signoff.' },
      { criterion: 'Cost per compliant localized module', weight: '15%', whatGoodLooksLike: 'Total localization cost falls as module volume increases without quality regression.', leftLens: 'Model dubbing spend across voice generation, QA passes, and rework rates by language.', rightLens: 'Model subtitle spend including translation, QA, and republish effort by language.' }
    ],
    buyingCriteria: [
      'Pilot one high-risk compliance lesson in at least two non-English languages before selecting default localization mode.',
      'Use the same legal/compliance reviewer panel to score both approaches with a shared defect rubric.',
      'Track post-launch comprehension by language (quiz misses tied to terminology) instead of relying on completion alone.',
      'Document fallback path: when to escalate from subtitles to dubbing for specific populations or risk classes.',
      'Choose the mode with lower total defect-correction effort over 30 days, not the fastest first publish.'
    ]
  },
  'scorm-authoring-vs-lms-native-builders': {
    decisionMatrix: [
      { criterion: 'Implementation speed for first production course', weight: '25%', whatGoodLooksLike: 'Team ships first approved course with tracking enabled and QA signoff inside planned launch window.', leftLens: 'Score how quickly IDs can author, package, and upload SCORM into your LMS with minimal rework.', rightLens: 'Score how quickly SMEs can build and publish directly in LMS-native builders without custom packaging steps.' },
      { criterion: 'Update velocity for recurring policy/process changes', weight: '25%', whatGoodLooksLike: 'Minor updates can be shipped weekly without breaking completions or version history.', leftLens: 'Measure cycle time for editing source files, republishing SCORM, and validating completion sync.', rightLens: 'Measure cycle time for in-LMS edits, approvals, and learner-visible rollouts across active cohorts.' },
      { criterion: 'Data fidelity and reporting depth', weight: '20%', whatGoodLooksLike: 'Learning records are consistent enough for compliance audits and manager coaching decisions.', leftLens: 'Validate SCORM/xAPI event capture, completion logic, and edge-case behavior in your target LMS.', rightLens: 'Validate native event granularity, export quality, and ability to track required assessment evidence.' },
      { criterion: 'Governance, version control, and handoffs', weight: '15%', whatGoodLooksLike: 'Ownership stays clear across IDs, admins, compliance reviewers, and regional stakeholders.', leftLens: 'Check authoring ownership model, source control discipline, and rollback process for packaged assets.', rightLens: 'Check permissioning granularity, approval routing, and audit logs inside LMS-native content workflows.' },
      { criterion: 'Total operating cost per maintained course', weight: '15%', whatGoodLooksLike: 'Cost and team effort decline as library size grows and refresh cadence increases.', leftLens: 'Model tool licensing + specialist authoring effort + QA overhead for each update cycle.', rightLens: 'Model LMS seat/feature cost + admin dependency + any limits on advanced interaction design.' }
    ],
    buyingCriteria: [
      'Pilot with one compliance-critical course and one high-change operational course before selecting your default path.',
      'Use a shared scorecard that includes re-publish effort, completion-data reliability, and reviewer minutes.',
      'Test at least one rollback scenario (bad publish) to validate recovery speed and audit defensibility.',
      'Document who owns authoring, QA, LMS admin steps, and post-launch reporting in your target operating model.',
      'Choose the option that minimizes long-term maintenance burden, not just first-launch speed.'
    ]
  },
  'ai-roleplay-simulators-vs-video-only-onboarding': {
    decisionMatrix: [
      { criterion: 'Time-to-ramp for customer-facing behavior', weight: '25%', whatGoodLooksLike: 'New hires can demonstrate target conversations before live customer exposure.', leftLens: 'Score how quickly roleplay scenarios produce measurable behavior improvement in week 1-2.', rightLens: 'Score how quickly video-only modules prepare hires without supervised practice loops.' },
      { criterion: 'Practice depth and feedback quality', weight: '25%', whatGoodLooksLike: 'Learners receive actionable feedback tied to rubric criteria, not generic completion signals.', leftLens: 'Evaluate scenario realism, coaching prompts, and retry loops by competency.', rightLens: 'Evaluate knowledge-check depth and whether managers can identify skill gaps from quiz data alone.' },
      { criterion: 'Manager coaching signal', weight: '20%', whatGoodLooksLike: 'Frontline managers can see who needs intervention and where.', leftLens: 'Measure analytics quality from simulated interactions (objection handling, policy phrasing, tone).', rightLens: 'Measure whether video completion + quiz scores provide enough detail for targeted coaching.' },
      { criterion: 'Operational overhead and governance', weight: '15%', whatGoodLooksLike: 'Program owners can maintain content updates without tool sprawl or unclear ownership.', leftLens: 'Assess scenario authoring effort, QA workflow, and reviewer signoff requirements.', rightLens: 'Assess update cadence, content drift risk, and compliance version control in static modules.' },
      { criterion: 'Cost per ramp-ready employee', weight: '15%', whatGoodLooksLike: 'Total enablement cost falls while quality outcomes improve across cohorts.', leftLens: 'Model simulator licensing + scenario maintenance against reduced manager shadowing time.', rightLens: 'Model lower content-production cost against longer ramp and higher live-call correction effort.' }
    ],
    buyingCriteria: [
      'Pilot one role with high conversation risk (sales, support, or compliance intake) before deciding default onboarding mode.',
      'Use the same manager rubric to score simulation performance and post-onboarding live performance.',
      'Track manager intervention time per new hire, not just module completion rates.',
      'Define trigger points for when video-only paths must escalate to practice-based simulation.',
      'Choose the approach with better 30-day ramp outcomes per unit of manager coaching effort.'
    ]
  },
  'synthesia-alternatives': {
    decisionMatrix: [
      { criterion: 'Speed to first publish', weight: '20%', whatGoodLooksLike: 'Team can publish first approved module in <5 business days.', synthesiaLens: 'Strong for avatar-led scripts; slower when teams need heavy scene-level edits.', alternativesLens: 'Prioritize options with fast template reuse and easy revision loops for SMEs.' },
      { criterion: 'Editing depth for L&D workflows', weight: '25%', whatGoodLooksLike: 'Instructional designers can refine pacing, visuals, and overlays without recreating scenes.', synthesiaLens: 'Clean workflow for standard formats, but complex edits can require workarounds.', alternativesLens: 'Favor tools with timeline-level control if your team iterates frequently.' },
      { criterion: 'Localization and language QA', weight: '20%', whatGoodLooksLike: 'Native-review pass is lightweight and pronunciation issues are fixable in-platform.', synthesiaLens: 'Broad language support; validate voice quality for domain vocabulary.', alternativesLens: 'Check glossary controls, voice cloning governance, and regional tone flexibility.' },
      { criterion: 'Governance + approvals', weight: '20%', whatGoodLooksLike: 'Version history, reviewer roles, and signoff checkpoints are explicit.', synthesiaLens: 'Evaluate workspace controls against compliance requirements.', alternativesLens: 'Some alternatives win on collaboration history and approval routing depth.' },
      { criterion: 'Operating cost at scale', weight: '15%', whatGoodLooksLike: 'Cost per published training minute declines as output rises.', synthesiaLens: 'Model cost against seat count + production volume.', alternativesLens: 'Benchmark total monthly spend including editing and localization tools.' }
    ],
    buyingCriteria: [
      'Run one identical pilot workflow across all shortlisted tools (same SOP, same reviewer panel, same deadline).',
      'Score each tool on revision turnaround time, not just first draft speed.',
      'Require a multilingual test clip if your org supports non-English learners.',
      'Validate export/integration path into LMS or knowledge base before procurement signoff.',
      'Tie final selection to 90-day operating model: owners, approval SLA, and update cadence.'
    ]
  }
};

const defaultComparisonContent = {
  decisionMatrix: [
    { criterion: 'Workflow fit', weight: '30%', whatGoodLooksLike: 'Publishing and updates stay fast under real team constraints.', synthesiaLens: 'Use this column to evaluate incumbent fit.', alternativesLens: 'Use this column to evaluate differentiation.' },
    { criterion: 'Review + governance', weight: '25%', whatGoodLooksLike: 'Approvals, versioning, and accountability are clear.', synthesiaLens: 'Check control depth.', alternativesLens: 'Check parity or advantage in review rigor.' },
    { criterion: 'Localization readiness', weight: '25%', whatGoodLooksLike: 'Multilingual delivery does not require full rebuilds.', synthesiaLens: 'Test language quality with real terminology.', alternativesLens: 'Test localization + reviewer workflows.' },
    { criterion: 'Cost to operate', weight: '20%', whatGoodLooksLike: 'Total effort and spend fall as output scales.', synthesiaLens: 'Model full-team cost.', alternativesLens: 'Model end-to-end production stack cost.' }
  ],
  buyingCriteria: [
    'Align stakeholders on one weighted scorecard before any demos.',
    'Use measurable pilot outcomes (cycle time, QA defects, completion impact).',
    'Document ownership and approval paths before rollout.',
    'Reassess fit after first production month with real usage data.'
  ]
};

const comparisonContent = { ...defaultComparisonContent, ...(comparisonContentBySlug[slug] || {}) };

const faq = [
  { q: 'What should L&D teams optimize for first?', a: 'Prioritize cycle-time reduction on one high-friction workflow, then expand only after measurable gains in production speed and adoption.' },
  { q: 'How long should a pilot run?', a: 'Two to four weeks is typically enough to validate operational fit, update speed, and stakeholder confidence.' },
  { q: 'How do we avoid a biased evaluation?', a: 'Use one scorecard, one test workflow, and the same review panel for every tool in the shortlist.' }
];

const jsonLd = {
  '@context': 'https://schema.org',
  '@graph': [
    { '@type': 'Article', headline: page.title, description: page.meta, mainEntityOfPage: canonical },
    { '@type': 'FAQPage', mainEntity: faq.map((item) => ({ '@type': 'Question', name: item.q, acceptedAnswer: { '@type': 'Answer', text: item.a } })) }
  ]
};
---
<Base title={page.title} description={page.meta} canonical={canonical} jsonLd={jsonLd}>
  <Breadcrumbs items={[{ label: 'Home', href: '/' }, { label: 'Compare', href: '/compare/' }, { label: page.title, href: `/compare/${slug}/` }]} />
  <h1>{page.title}</h1>
  <p class="muted">{page.intro} Use this route to decide faster with an implementation-led lens instead of a feature checklist.</p>

  <h2>Practical comparison framework</h2>
  <ol>
    <li><strong>Workflow fit:</strong> Can your team publish and update training content quickly?</li>
    <li><strong>Review model:</strong> Are approvals and versioning reliable for compliance-sensitive content?</li>
    <li><strong>Localization:</strong> Can you support multilingual or role-specific variants without rework?</li>
    <li><strong>Total operating cost:</strong> Does the tool reduce weekly effort for content owners and managers?</li>
  </ol>

  <h2>Decision matrix</h2>
  <p class="muted" style="margin-top:-0.4rem;">On mobile, use the card view below for faster side-by-side scoring.</p>
  <div class="matrix-table-wrap" aria-label="Desktop decision matrix table">
    <table class="matrix-table">
      <thead>
        <tr>
          <th>Criterion</th>
          <th>Weight</th>
          <th>What good looks like</th>
          <th>{comparisonLabels.left} lens</th>
          <th>{comparisonLabels.right} lens</th>
        </tr>
      </thead>
      <tbody>
        {comparisonContent.decisionMatrix.map((row) => (
          <tr>
            <td><strong>{row.criterion}</strong></td>
            <td>{row.weight}</td>
            <td>{row.whatGoodLooksLike}</td>
            <td>{row.leftLens || row.synthesiaLens}</td>
            <td>{row.rightLens || row.alternativesLens}</td>
          </tr>
        ))}
      </tbody>
    </table>
  </div>

  <div class="matrix-mobile" aria-label="Mobile decision matrix cards">
    {comparisonContent.decisionMatrix.map((row) => (
      <article class="card matrix-card">
        <h3>{row.criterion}</h3>
        <p><strong>Weight:</strong> {row.weight}</p>
        <p><strong>What good looks like:</strong> {row.whatGoodLooksLike}</p>
        <p><strong>{comparisonLabels.left} lens:</strong> {row.leftLens || row.synthesiaLens}</p>
        <p><strong>{comparisonLabels.right} lens:</strong> {row.rightLens || row.alternativesLens}</p>
      </article>
    ))}
  </div>

  <h2>Buying criteria before final selection</h2>
  <ul>
    {comparisonContent.buyingCriteria.map((item) => <li>{item}</li>)}
  </ul>

  <h2>Related tools in this directory</h2>
  <div class="grid">
    {candidates.map((tool) => (
      <article class="card">
        <h3><a href={`/tool/${tool.slug}/`}>{tool.name}</a></h3>
        <p>{tool.summary}</p>
      </article>
    ))}
  </div>

  <section class="section" aria-labelledby="next-steps-heading">
    <h2 id="next-steps-heading">Next steps</h2>
    <div class="chips chips-nav">
      <a class="chip" href="/solutions/">Browse solution pages</a>
      <a class="chip" href="/solutions/sop-to-video-training/">SOP-to-video implementation route</a>
      <a class="chip" href="/solutions/compliance-training-content-creation/">Compliance content route</a>
      <a class="chip" href="/categories/">Explore categories</a>
    </div>
  </section>

  <section class="section" aria-labelledby="faq-heading">
    <h2 id="faq-heading">FAQ</h2>
    {faq.map((item) => (
      <>
        <h3>{item.q}</h3>
        <p>{item.a}</p>
      </>
    ))}
  </section>

  <style>
    .matrix-table-wrap {
      overflow-x: auto;
      display: block;
    }

    .matrix-table {
      width: 100%;
      border-collapse: collapse;
      min-width: 760px;
      background: #fff;
      border: 2px solid #18181b;
    }

    .matrix-table th,
    .matrix-table td {
      border: 1px solid #d4d4d8;
      padding: 0.7rem;
      text-align: left;
      vertical-align: top;
    }

    .matrix-table th {
      background: #dcfce7;
      font-size: 0.9rem;
    }

    .matrix-mobile {
      display: none;
      gap: 0.75rem;
      margin-top: 0.7rem;
    }

    .matrix-card {
      box-shadow: none;
      border-top-width: 4px;
      padding: 1rem;
    }

    .matrix-card h3 {
      margin-bottom: 0.65rem;
      font-size: 1.12rem;
    }

    .matrix-card p {
      margin-bottom: 0.6rem;
      max-width: none;
      font-size: 0.95rem;
    }

    .matrix-card p:last-child {
      margin-bottom: 0;
    }

    @media (max-width: 760px) {
      .matrix-table-wrap {
        display: none;
      }

      .matrix-mobile {
        display: grid;
      }
    }
  </style>
</Base>
